<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <style>
        section {
            display: block;
        }
    </style>
</head>

<body style="font-family: Arial, Helvetica, sans-serif">

    <div style="display:none">
        \( \newcommand{\argminI}{\mathop{\mathrm{argmin}}\nolimits} \DeclareMathOperator*{\argminA}{arg\,min} \newcommand{\argminD}{\arg\!\min}
        \)
    </div>
    <h2>Notation and definitions</h2>
    <ul>
        <li>A matrix is denoted by a bold letter: \(\mathbf{X}\)</li>
        <li>For a matrix \(\mathbf{X}\), the element in the \(i\)
            <sup>th</sup> row and \(j\)
            <sup>th</sup> column of \(\mathbf{X}\) is denoted \(x_{ij}\). An \(N \times p\) matrix \(\mathbf{X}\) may also be
            denoted: $$ \mathbf{X} = \begin{bmatrix} x_{11} & x_{12} & \dots & x_{1p} \\ x_{21} & x_{22} & \dots & x_{2p}
            \\ \vdots & \vdots & \ddots& \vdots\\ x_{N1} & x_{N2} & \dots & x_{Np} \end{bmatrix} $$
        </li>
        <li>In general, row and column vectors are denoted by a single lower case bold letter: \(\mathbf{a}\) .</li>
        <li>The row vector representing the \(i\)
            <sup>th</sup> row of a matrix \(\mathbf{X}\) is denoted \(\mathbf{x}_i\).</li>
        <li>The column vector representing the \(i\)
            <sup>th</sup> column of a matrix \(\mathbf{X}\) is denoted by a capital letter: \(X_i\)</li>
        <li>Given an \(N \times p\)
            <a href="https://en.wikipedia.org/wiki/Data_matrix_(multivariate_statistics)">data matrix</a> \(\mathbf{X}\), each row represents an individual observation. The columns are referred to as
            <em>feature variables</em>.</li>
        <li> The sample mean of the \(j\)
            <sup>th</sup> feature variable of a data matrix \(\mathbf{X}\) is denoted \(\mu_j\)
        </li>
        <li>The sample variance of  the \(j\)<sup>th</sup> feature variable of a data matrix \(\mathbf{X}\) is denoted by \(\mathrm{var(X_j)}\) or \(\sigma_j\). 
            $$ \mathrm{var(X_j)} = \sigma_j = \sum_{i=1}^{N}{(x_{ij}-
            \mu_j)^2} $$
        </li>
    </ul>

    <h2>Principle Components</h2>
    <p>Given an \(N \times p\)
        <a href="https://en.wikipedia.org/wiki/Data_matrix_(multivariate_statistics)">data matrix</a> \(\mathbf{X}\), we have \(p\) feature variables, each of which corresponds to a column of \(\mathbf{X}
        = \Big[ X_1 X_2 ...X_p \Big] \). The objective of principle components analysis is to find a smaller set of variables
        with the maximal variance, \(C_i\) that contain the same information but are uncorrelated with each other.</p>

    <p>The objective is to find a transformation from \(X\) to a matrix \(C\), which is made up of the
        <em>principle components</em> of \(X\).</p>
    <p>The principle components are the \(N \times 1\) column vectors \(C_i = XE_i\) that satisfy: $$ \begin{equation} E_i =
        \underset{E_i}{\mathrm{arg\min}} \left\{ \mathrm{var(C_i)} \right\} \\ \mathrm{cov}(C_i, C_j) = 0 \quad \forall j
        \lt i \\ \lVert E_1 \rVert = 1 \end{equation} $$

    </p> Maximizing variance $$ \begin{align} var(C_i) = var(XE_i) &= \sum_{n=1}^{N}{(\mathbf{x}_n E_i)^2} \\ &= (XE_i)^T
    XE_i \\ &= E_i^T X^T XE_i \end{align} $$
</body>

</html>